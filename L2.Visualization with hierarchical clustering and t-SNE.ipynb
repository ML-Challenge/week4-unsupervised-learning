{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/ML-Challenge/week4-unsupervised-learning/blob/master/L2.Visualization%20with%20hierarchical%20clustering%20and%20t-SNE.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" /></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lesson, we'll learn about two unsupervised learning techniques for data visualization, hierarchical clustering and t-SNE. Hierarchical clustering merges the data samples into ever-coarser clusters, yielding a tree visualization of the resulting cluster hierarchy. t-SNE maps the data samples into 2d space so that the proximity of the samples to one another can be visualized."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-15T16:11:39.471122Z",
     "start_time": "2020-02-15T16:11:39.458157Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Download lesson datasets\n",
    "# Required if you're using Google Colab\n",
    "#!wget \"https://github.com/ML-Challenge/week4-unsupervised-learning/raw/master/datasets.zip\"\n",
    "#!unzip -o datasets.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-15T16:11:39.955835Z",
     "start_time": "2020-02-15T16:11:39.474129Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Import utils\n",
    "# We'll be using this module throughout the lesson\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-15T16:11:40.185260Z",
     "start_time": "2020-02-15T16:11:39.957829Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Import dependencies\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "# and setting the size of all plots.\n",
    "plt.rcParams['figure.figsize'] = [20, 10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Visualizing hierarchies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "A huge part of our work as a data scientist will be the communication of our insights to other people. Visualizations are an excellent way to share our findings, particularly with a non-technical audience. \n",
    "\n",
    "In this lesson, we'll learn about two unsupervised learning techniques for visualization: t-SNE and hierarchical clustering.\n",
    "\n",
    "t-SNE, which we'll consider later, creates a 2D map of any dataset, and conveys useful information about the proximity of the samples to one another. First up, however, let's learn about hierarchical clustering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## A hierarchy of groups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "We've already seen many hierarchical clusterings in the real world. For example, living things can be organised into small narrow groups, like humans, apes, snakes and lizards, or into larger, broader groups like mammals and reptiles, or even broader groups like animals and plants. These groups are contained in one another, and form a hierarchy.\n",
    "\n",
    "![Animals](assets/2-1.png)\n",
    "\n",
    "Analogously, hierarchical clustering arranges samples into a hierarchy of clusters. Hierarchical clustering can organise any sort of data into a hierarchy, not just samples of plants and animals."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Eurovision scoring dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Let's consider a new type of dataset, describing how countries scored performances at the Eurovision 2016 song contest. The data is arranged in a rectangular array, where the rows of the array show how many points a country gave to each song. The `samples` in this case are the countries.\n",
    "\n",
    "![Eurovision](assets/2-2.png)\n",
    "\n",
    "The result of applying hierarchical clustering to the Eurovision scores can be visualized as a tree-like diagram called a dendrogram.\n",
    "\n",
    "![Eurovision](assets/2-3.png)\n",
    "\n",
    "This single picture reveals a great deal of information about the voting behavior of countries at the Eurovision. The dendrogram groups the countries into larger and larger clusters, and many of these clusters are immediately recognizable as containing countries that are close to one another\n",
    "geographically, or that have close cultural or political ties, or that belong to single language group.\n",
    "\n",
    "So hierarchical clustering can produce great visualizations. But how does it work?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Hierarchical clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Hierarchical clustering proceeds in steps. In the beginning, every country is its own cluster - so there are as many clusters as there are countries! At each step, the two closest clusters are merged. This decreases the number of clusters, and eventually, there is only one cluster left, and it contains all the countries.\n",
    "\n",
    "This process is actually a particular type of hierarchical clustering called `agglomerative clustering` - there is also `divisive clustering`, which works the other way around.\n",
    "\n",
    "We haven't defined yet what it means for two clusters to be close, but we'll revisit that later on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The entire process of the hierarchical clustering is encoded in the dendrogram. At the bottom, each country is in a cluster of its own. The clustering then proceeds from the bottom up. Clusters are represented as vertical lines. A joining of lines indicates a merging of clusters.\n",
    "\n",
    "To understand better, let's zoom in and look at just one part of this dendrogram.\n",
    "\n",
    "![Zoom](assets/2-4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "In the beginning, there are six clusters, each containing only one country.\n",
    "\n",
    "![Zoom](assets/2-5.png)\n",
    "\n",
    "The first merging is here, where the clusters containing Cyprus and Greece are merged together in a single cluster.\n",
    "\n",
    "![Zoom](assets/2-6.png)\n",
    "\n",
    "Later on, this new cluster is merged with the cluster containing Bulgaria.\n",
    "\n",
    "![Zoom](assets/2-7.png)\n",
    "\n",
    "Shortly after that, the clusters containing Moldova and Russia are merged, \n",
    "\n",
    "![Zoom](assets/2-8.png)\n",
    "\n",
    "which later is in turn merged with the cluster containing Armenia.\n",
    "\n",
    "![Zoom](assets/2-9.png)\n",
    "\n",
    "Later still, the two big composite clusters are merged together.\n",
    "\n",
    "![Zoom](assets/2-10.png)\n",
    "\n",
    "This process continues until there is only one cluster left, and it contains all the countries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Hierarchical clustering with SciPy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "We'll use functions from `scipy` to perform a hierarchical clustering on the array of scores. For the dendrogram, we'll also need a list of country names.\n",
    "\n",
    "Firstly, import the linkage and dendrogram functions.\n",
    "\n",
    "```\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scipy.cluster.hierarchy import linkage, dendrogram\n",
    "```\n",
    "\n",
    "Then, apply the linkage function to the sample array.\n",
    "\n",
    "```\n",
    "mergings = linkage(utils.eurovision, method='complete')\n",
    "```\n",
    "\n",
    "Its the linkage function that performs the hierarchical clustering. Notice there is an extra method parameter - we'll cover that here.\n",
    "\n",
    "Now pass the output of linkage to the dendrogram function, specifying the list of country names as the labels parameter.\n",
    "\n",
    "```\n",
    "dendrogram(mergings, labels = utils.country_names, leaf_rotation = 90, leaf_font_size = 10)\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "Next, we'll learn how to extract information from a hierarchical clustering. But for now, let's see what hierarchical clustering can do with some real-world datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## How many merges?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "If there are 5 data samples, how many merge operations will occur in a hierarchical clustering? To help answer this question, think back to the example of hierarchical clustering using 6 countries. How many merge operations did that example have?\n",
    "\n",
    "**Posible Answers**\n",
    "\n",
    "1. 4 merges\n",
    "2. 5 merges\n",
    "3. This can't be known in advance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-15T16:11:40.201183Z",
     "start_time": "2020-02-15T16:11:40.187219Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Enter 1, 2 or 3 as parameter\n",
    "utils.how_many_merges(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Hierarchical clustering of the grain data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The SciPy `linkage()` function performs hierarchical clustering on an array of samples. Let's use the `linkage()` function to obtain a hierarchical clustering of the grain samples, and use `dendrogram()` to visualize the result. A sample of the grain measurements is provided in the array `grain`, while the variety of each grain sample is given by the list `varieties`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-15T16:11:40.342398Z",
     "start_time": "2020-02-15T16:11:40.204178Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Perform the necessary imports\n",
    "from scipy.cluster.hierarchy import linkage, dendrogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-15T16:11:40.358289Z",
     "start_time": "2020-02-15T16:11:40.344325Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Calculate the linkage: mergings\n",
    "mergings = linkage(utils.grain, method='complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-15T16:11:46.250535Z",
     "start_time": "2020-02-15T16:11:40.361280Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Plot the dendrogram, using varieties as labels\n",
    "dendrogram(mergings,\n",
    "           labels=utils.varieties,\n",
    "           leaf_rotation=90,\n",
    "           leaf_font_size=6,\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Dendrograms are a great way to illustrate the arrangement of the clusters produced by hierarchical clustering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Hierarchies of stocks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "In lesson 1, we used k-means clustering to cluster companies according to their stock price movements. Now, we'll perform hierarchical clustering of the companies. We are given a NumPy array of price movements `movements`, where the rows correspond to companies, and a list of the company names `companies`. SciPy hierarchical clustering doesn't fit into a sklearn pipeline, so we'll need to use the `normalize()` function from `sklearn.preprocessing` instead of `Normalizer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-15T16:11:46.518914Z",
     "start_time": "2020-02-15T16:11:46.252502Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Import normalize\n",
    "from sklearn.preprocessing import normalize "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-15T16:11:46.534805Z",
     "start_time": "2020-02-15T16:11:46.520846Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Normalize the movements: normalized_movements\n",
    "normalized_movements = normalize(utils.movements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-15T16:11:46.550798Z",
     "start_time": "2020-02-15T16:11:46.536800Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Calculate the linkage: mergings\n",
    "mergings = linkage(normalized_movements, method='complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-15T16:11:48.285687Z",
     "start_time": "2020-02-15T16:11:46.552758Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Plot the dendrogram\n",
    "dendrogram(mergings, labels=utils.companies, leaf_rotation=90, leaf_font_size=10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "We can produce great visualizations such as this with hierarchical clustering, but it can be used for more than just visualizations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Cluster labels in hierarchical clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Previously, we employed hierarchical clustering to create a great visualization of the voting behavior at the Eurovision. But hierarchical clustering is not only a visualization tool.\n",
    "\n",
    "Now, we'll learn how to extract the clusters from intermediate stages of a hierarchical clustering. The cluster labels for these intermediate clusterings can then be used in further computations, such as cross tabulations, just like the cluster labels from k-means.\n",
    "\n",
    "An intermediate stage in the hierarchical clustering is specified by choosing a height on the dendrogram. For example, choosing a height of 15 defines a clustering in which Bulgaria, Cyprus and Greece are in one cluster, Russia and Moldova are in another, and Armenia is in a cluster on its own.\n",
    "\n",
    "![Zoom](assets/2-5_1.png)\n",
    "\n",
    "But what is the meaning of the height? \n",
    "\n",
    "The y-axis of the dendrogram encodes the distance between merging clusters. For example, the distance between the cluster containing Cyprus and the cluster containing Greece was approximately 6 when they were merged into a single cluster. When this new cluster was merged with the cluster containing Bulgaria, the distance between them was 12. \n",
    "\n",
    "So the height that specifies an intermediate clustering corresponds to a distance. This specifies that the hierarchical clustering should stop merging clusters when all clusters are at least this far apart."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "**Distance between clusters**\n",
    "\n",
    "The distance between two clusters is measured using a `linkage method`. In our example, we used complete linkage, where the distance between two clusters is the maximum of the distances between their samples. This was specified via the method parameter. There are many other linkage methods, and we'll see in the examples that different linkage methods give different hierarchical clusterings!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Extracting cluster labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The cluster labels for any intermediate stage of the hierarchical clustering can be extracted using the `fcluster` function. Let's try it out, specifying the height of 15.\n",
    "\n",
    "```\n",
    "from scipy.cluster.hierarchy import linkage, dendrogram\n",
    "\n",
    "mergings = linkage(utils.eurovision, method='complete')\n",
    "```\n",
    "\n",
    "After performing the hierarchical clustering of the Eurovision data, import the fcluster function.\n",
    "\n",
    "```\n",
    "from scipy.cluster.hierarchy import fcluster\n",
    "```\n",
    "\n",
    "Then pass the result of the linkage function to the fcluster function, specifying the height as the second argument.\n",
    "\n",
    "```\n",
    "labels = fcluster(mergings, 15, criterion='distance')\n",
    "```\n",
    "\n",
    "This returns a numpy array containing the cluster labels for the countries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Aligning cluster labels with country names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "To inspect cluster labels, let's use a DataFrame to align the labels with the country names.\n",
    "\n",
    "Firstly, import pandas, then create the dataframe, and then sort by cluster label, printing the result.\n",
    "\n",
    "```\n",
    "import pandas as pd\n",
    "\n",
    "pairs = pd.DataFrame({'labels': labels, 'countries': utils.country_names})\n",
    "\n",
    "print(pairs.sort_values('labels')\n",
    "```\n",
    "\n",
    "|      | countries | labels |\n",
    "| :--- | --------: | -----: |\n",
    "| 5    | Belarus   | 1      |\n",
    "| 40   | Ukraine   | 1      |\n",
    "| 17   | Georgia   | 1      |\n",
    "| ---  |           |        |\n",
    "| 36   | Spain     | 5      |\n",
    "| 8    | Bulgaria  | 6      |\n",
    "| 19   | Greece    | 6      |\n",
    "| 10   | Cyprus    | 6      |\n",
    "| 28   | Moldova   | 7      |\n",
    "| ---  |           |        |\n",
    "\n",
    "As expected, the cluster labels group Bulgaria, Greece and Cyprus in the same cluster. But do note that the `scipy` cluster labels start at 1, not at 0 like they do in scikit-learn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Which clusters are closest?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "We learned that the linkage method defines how the distance between clusters is measured. In complete linkage, the distance between clusters is the distance between the furthest points of the clusters. In single linkage, the distance between clusters is the distance between the closest points of the clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Consider the three clusters in the diagram. Which of the following statements are true?\n",
    "\n",
    "![Clusters Riddle](assets/cluster_linkage_riddle.png)\n",
    "\n",
    "A. In single linkage, Cluster 3 is the closest to Cluster 2.<br>\n",
    "B. In complete linkage, Cluster 1 is the closest to Cluster 2.\n",
    "\n",
    "**Posible Answers**\n",
    "\n",
    "1. Neither A nor B\n",
    "2. A only\n",
    "3. Both A and B."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-15T16:11:48.301612Z",
     "start_time": "2020-02-15T16:11:48.287646Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Enter 1, 2 or 3 as parameter\n",
    "utils.clusters_linkage_riddle(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Different linkage, different hierarchical clustering!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "We saw a hierarchical clustering of the voting countries at the Eurovision song contest using 'complete' linkage. Now, let's perform a hierarchical clustering of the voting countries with 'single' linkage, and compare the resulting dendrogram with the one before. Different linkage, different hierarchical clustering!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-15T16:11:48.316570Z",
     "start_time": "2020-02-15T16:11:48.304604Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Calculate the linkage: mergings\n",
    "mergings = linkage(utils.eurovision_samples, method='single')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-15T16:11:48.815895Z",
     "start_time": "2020-02-15T16:11:48.319561Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Plot the dendrogram\n",
    "dendrogram(mergings, labels=utils.country_names, leaf_rotation=90, leaf_font_size=10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Performing single linkage hierarchical clustering produces a different dendrogram!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Extracting the cluster labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "We will now use the `fcluster()` function to extract the cluster labels for intermediate clustering, and compare the labels with the grain varieties using a cross-tabulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-15T16:11:48.830911Z",
     "start_time": "2020-02-15T16:11:48.817892Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Perform the necessary imports\n",
    "from scipy.cluster.hierarchy import fcluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-15T16:11:48.846847Z",
     "start_time": "2020-02-15T16:11:48.833847Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Calculate the linkage: mergings\n",
    "mergings = linkage(utils.grain, method='complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-15T16:11:48.861804Z",
     "start_time": "2020-02-15T16:11:48.849820Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Use fcluster to extract labels: labels\n",
    "labels = fcluster(mergings, 6, criterion='distance')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-15T16:11:48.877800Z",
     "start_time": "2020-02-15T16:11:48.863793Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Create a DataFrame with labels and varieties as columns: df\n",
    "df = pd.DataFrame({'labels': labels, 'varieties': utils.varieties})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-15T16:11:48.909646Z",
     "start_time": "2020-02-15T16:11:48.879728Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Create crosstab: ct\n",
    "ct = pd.crosstab(df['labels'], df['varieties'])\n",
    "\n",
    "# Display ct\n",
    "print(ct)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Fantastic - we've now mastered the fundamentals of k-Means and agglomerative hierarchical clustering. Next, we'll learn about t-SNE, which is a powerful tool for visualizing high dimensional data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# t-SNE for 2-dimensional maps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check out an unsupervised learning method for visualization called t-SNE.\n",
    "t-SNE stands for `t-distributed stochastic neighbor embedding`. It has a complicated name, but it serves a very simple purpose.\n",
    "\n",
    "It maps samples from their high-dimensional space into a 2- or 3-dimensional space so they can visualized. While some distortion is inevitable, t-SNE does a great job of approximately representing the distances between the samples. For this reason, t-SNE is an invaluable visual aid for understanding a dataset.\n",
    "\n",
    "To see what sorts of insights are possible with t-SNE, let's look at how it performs on the iris dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## t-SNE on the iris dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The iris samples are in a four dimensional space, where each dimension corresponds to one of the four iris measurements, such as petal length and petal width. Now t-SNE was given only the measurements of the iris samples.  In particular it wasn't given any information about the three species of iris. But if we color the species differently on the scatter plot, we see that t-SNE has kept the species separate.\n",
    "\n",
    "![t-SNE](assets/2-11.png)\n",
    "\n",
    "This scatter plot gives us a new insight, however. We learn that there are two iris species, versicolor and virginica, whose samples are close together in space. So it could happen that the iris dataset appears to have two clusters, instead of three.\n",
    "\n",
    "This is compatible with our previous examples using k-means, where we saw that a  clustering with 2 clusters also had relatively low inertia, meaning tight clusters.\n",
    "\n",
    "![t-SNE](assets/1-3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## t-SNE in sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "t-SNE is available in scikit-learn, but it works a little differently to the fit/transform components we've already met. Let's see it in action on the iris dataset. The samples are in a 2-dimensional numpy array, and there is a list giving the species of each sample.\n",
    "\n",
    "To start with, import TSNE and create a TSNE object.\n",
    "\n",
    "```\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "model = TSNE(learning_rate=100)\n",
    "```\n",
    "\n",
    "Apply the `fit_transform` method to the samples, and then make a scatter plot of the result, coloring the points using the species.\n",
    "\n",
    "```\n",
    "transformed = model.fit_transform(samples)\n",
    "\n",
    "xs = transformed[:,0]\n",
    "ys = transformed[:,1]\n",
    "\n",
    "plt.scatter(xs, ys, c=species)\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "There are two aspects that deserve special attention: the fit_transform method, and the learning rate.\n",
    "\n",
    "* t-SNE only has a `fit_transform method`: as we might expect, the fit_transform method simultaneously fits the model and transforms the data. However, t-SNE does not have separate fit and transform methods. This means that we can't extend a t-SNE map to include new samples. Instead, we have to start over each time.\n",
    "* The second thing to notice is the learning rate: the learning rate makes the use of t-SNE more complicated than some other techniques. We may need to try different learning rates for different datasets. It is clear, however, when we've made a bad choice, because all the samples appear bunched together in the scatter plot. Normally it's enough to try a few values between 50 and 200.\n",
    "\n",
    "A final thing to be aware of is that the axes of a t-SNE plot do not have any interpretable meaning.\n",
    "In fact, they are different every time t-SNE is applied, even on the same data. For example, here are three t-SNE plots of the scaled Piedmont wine samples, generated using the same code.\n",
    "\n",
    "![tSNE Wine](assets/2-12.png)\n",
    "\n",
    "Note that while the orientation of the plot is different each time, the three wine varieties, represented here using colors, have the same position relative to one another."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## t-SNE visualization of grain dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "In this example, we'll apply t-SNE to the grain samples data and inspect the resulting t-SNE features using a scatter plot. We will use the grain samples and a list `variety_numbers` giving the variety number of each grain sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-15T16:11:49.003505Z",
     "start_time": "2020-02-15T16:11:48.911643Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Import TSNE\n",
    "from sklearn.manifold import TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-15T16:11:49.019418Z",
     "start_time": "2020-02-15T16:11:49.005431Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Create a TSNE instance: model\n",
    "model = TSNE(learning_rate=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-15T16:11:49.910027Z",
     "start_time": "2020-02-15T16:11:49.024380Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Apply fit_transform to samples: tsne_features\n",
    "tsne_features = model.fit_transform(utils.grain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-15T16:11:49.925985Z",
     "start_time": "2020-02-15T16:11:49.916012Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Select the 0th feature: xs\n",
    "xs = tsne_features[:,0]\n",
    "\n",
    "# Select the 1st feature: ys\n",
    "ys = tsne_features[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-15T16:12:18.990842Z",
     "start_time": "2020-02-15T16:12:18.790344Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Scatter plot, coloring by variety_numbers\n",
    "plt.scatter(xs,ys,c=utils.variety_numbers)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "As we can see, the t-SNE visualization manages to separate the 3 varieties of grain samples. But how will it perform on the stock data? We'll find out in the next example!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## A t-SNE map of the stock market"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "t-SNE provides great visualizations when the individual samples can be labeled. In this example, we'll apply t-SNE to the company stock price data. A scatter plot of the resulting t-SNE features, labeled by the company names, gives us a map of the stock market! The stock price movements for each company are available as the array `normalized_movements`. The list companies gives the name of each company."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-15T16:14:30.144537Z",
     "start_time": "2020-02-15T16:14:30.127583Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Create a TSNE instance: model\n",
    "model = TSNE(learning_rate=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-15T16:14:36.581388Z",
     "start_time": "2020-02-15T16:14:35.950095Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Apply fit_transform to normalized_movements: tsne_features\n",
    "tsne_features = model.fit_transform(normalized_movements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-15T16:14:40.689081Z",
     "start_time": "2020-02-15T16:14:40.679107Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Select the 0th feature: xs\n",
    "xs = tsne_features[:,0]\n",
    "\n",
    "# Select the 1th feature: ys\n",
    "ys = tsne_features[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-15T16:15:01.749150Z",
     "start_time": "2020-02-15T16:15:01.244491Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Scatter plot\n",
    "plt.scatter(xs, ys, alpha=0.5)\n",
    "\n",
    "# Annotate the points\n",
    "for x, y, company in zip(xs, ys, utils.companies):\n",
    "    plt.annotate(company, (x, y), fontsize=10, alpha=0.75)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "It's visualizations such as this that make t-SNE such a powerful tool for extracting quick insights from high dimensional data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**[Week 4 - Unsupervised Learning](https://radu-enuca.gitbook.io/ml-challenge/unsupervised-learning)**\n",
    "\n",
    "*Have questions or comments? Visit the ML Challenge Mattermost Channel.*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
